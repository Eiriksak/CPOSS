{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the primary type of crime\n",
    "\n",
    "Useful links\n",
    "* https://towardsdatascience.com/pyspark-demand-forecasting-data-science-project-dae14b5319cc\n",
    "* https://towardsdatascience.com/machine-learning-with-pyspark-and-mllib-solving-a-binary-classification-problem-96396065d2aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "sc = pyspark.SparkContext(master='spark://192.168.11.239:7077', appName='type_predicter')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "from pyspark.sql.types import StringType\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F #avoid conflicts with regular python functions\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset and look at the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Case Number: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Block: string (nullable = true)\n",
      " |-- IUCR: string (nullable = true)\n",
      " |-- Primary Type: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- Arrest: string (nullable = true)\n",
      " |-- Domestic: string (nullable = true)\n",
      " |-- Beat: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- Ward: string (nullable = true)\n",
      " |-- Community Area: string (nullable = true)\n",
      " |-- FBI Code: string (nullable = true)\n",
      " |-- X Coordinate: string (nullable = true)\n",
      " |-- Y Coordinate: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Updated On: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.csv(\"/datasets/district11.csv\", header='true')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Figure out which features we will be able to provide during a live prediction. We will probably only have time and an uncertain location description to base our prediction on.\n",
    "\n",
    "First step is to divide the date column into smaller subsets of time. Which one we should keep will be decided later when we tune the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(\"/datasets/district11.csv\", header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df\n",
    "       .withColumn('Timestamps', F.to_timestamp(\"Date\", 'MM/dd/yyyy hh:mm:ss a'))\n",
    "       .withColumn('Day', F.to_date(\"Date\", 'MM/dd/yyyy hh:mm:ss a'))\n",
    "       .withColumn(\"Month\", F.month(\"Day\"))\n",
    "       .withColumn(\"Hour\", F.hour(\"Timestamps\"))\n",
    "       .withColumn(\"Minute\", F.minute(\"Timestamps\"))\n",
    "       .withColumn(\"DayOfMonth\", F.dayofmonth(\"Day\"))\n",
    "       .withColumn(\"DayOfYear\", F.dayofyear(\"Day\"))\n",
    "       .withColumn(\"DayOfWeek\", F.dayofweek(\"Day\"))\n",
    "       .withColumn('WeekOfYear', F.weekofyear(\"Day\"))\n",
    "       .withColumn('Quarter', F.quarter(\"Timestamps\"))\n",
    "       \n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+----------+---------+---------+----+------+----------+-------+\n",
      "|       Day|Year|Month|DayOfMonth|DayOfYear|DayOfWeek|Hour|Minute|WeekOfYear|Quarter|\n",
      "+----------+----+-----+----------+---------+---------+----+------+----------+-------+\n",
      "|2007-01-01|2007|    1|         1|        1|        2|   0|     1|         1|      1|\n",
      "|2008-01-01|2008|    1|         1|        1|        3|   0|     1|         1|      1|\n",
      "|2017-09-01|2017|    9|         1|      244|        6|   9|     0|        35|      3|\n",
      "|2018-02-04|2018|    2|         4|       35|        1|  15|    25|         5|      1|\n",
      "|2014-11-01|2014|   11|         1|      305|        7|   9|     0|        44|      4|\n",
      "+----------+----+-----+----------+---------+---------+----+------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Day\",\"Year\",\"Month\",\"DayOfMonth\",\"DayOfYear\",\"DayOfWeek\",\"Hour\",\"Minute\",\"WeekOfYear\",\"Quarter\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact location description such as longitude and latitude will be hard to provide. However, we should be able to provide down to Beat areas etc. Also, in the best case we might have an intuition of the location description. \n",
    "\n",
    "Next step will be to drop unrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Day\",\"Year\",\"Month\",\"Hour\",\"Minute\",\"DayOfMonth\",\"DayOfYear\",\"DayOfWeek\",\"WeekOfYear\",\"Quarter\",\n",
    "       \"Beat\",\"Location Description\",\"Primary Type\"]\n",
    "df = df.select(*cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets see if some columns contains N/A values. Ignore columns with date type since they will fail in this case, and the date derivatives would have failed earlier on.\n",
    "Inspired by https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+------+----------+---------+---------+----------+-------+----+--------------------+------------+\n",
      "|Year|Month|Hour|Minute|DayOfMonth|DayOfYear|DayOfWeek|WeekOfYear|Quarter|Beat|Location Description|Primary Type|\n",
      "+----+-----+----+------+----------+---------+---------+----------+-------+----+--------------------+------------+\n",
      "|   0|    0|   0|     0|         0|        0|        0|         0|      0|   0|                 213|           0|\n",
      "+----+-----+----+------+----------+---------+---------+----------+-------+----+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) \\\n",
    "           for (c,col_type) in df.dtypes if col_type not in ('date')]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only column with N/A value in this case is Location Description. We can handle this by filling missing values or removing those rows. (In case Primary Type has N/A value, drop those rows as this is the target label). Lets look at how the data is distributed over the Location Description column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|  122|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(F.countDistinct(F.col(\"Location Description\")).alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 122 distinct location descriptions. Lets take a look at the top 30 ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Location Description| count|\n",
      "+--------------------+------+\n",
      "|              STREET|129033|\n",
      "|            SIDEWALK| 88343|\n",
      "|           RESIDENCE| 56639|\n",
      "|           APARTMENT| 46895|\n",
      "|               ALLEY| 15055|\n",
      "|               OTHER| 14240|\n",
      "|SCHOOL, PUBLIC, B...|  8945|\n",
      "|RESIDENCE PORCH/H...|  8443|\n",
      "|VEHICLE NON-COMME...|  7894|\n",
      "|PARKING LOT/GARAG...|  7780|\n",
      "|         GAS STATION|  6010|\n",
      "|RESIDENTIAL YARD ...|  5378|\n",
      "|     VACANT LOT/LAND|  5208|\n",
      "|        CTA PLATFORM|  4836|\n",
      "|POLICE FACILITY/V...|  4056|\n",
      "|  GROCERY FOOD STORE|  3931|\n",
      "|  SMALL RETAIL STORE|  3705|\n",
      "|    RESIDENCE-GARAGE|  2847|\n",
      "|CHA PARKING LOT/G...|  2837|\n",
      "|          RESTAURANT|  2777|\n",
      "|       PARK PROPERTY|  2732|\n",
      "|CHA HALLWAY/STAIR...|  1968|\n",
      "|SCHOOL, PUBLIC, G...|  1894|\n",
      "|  ABANDONED BUILDING|  1805|\n",
      "|       CHA APARTMENT|  1622|\n",
      "|           CTA TRAIN|  1559|\n",
      "|             CTA BUS|  1227|\n",
      "|COMMERCIAL / BUSI...|  1075|\n",
      "|   CONVENIENCE STORE|  1034|\n",
      "| TAVERN/LIQUOR STORE|   985|\n",
      "+--------------------+------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Location Description\").count().sort(F.col(\"count\").desc()).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell that most of the descriptions are among these top results. Since we only has 213 missing values in this case, lets just remove them. If we were to fill them, we could use the most frequent one (STREET), since it dominates the count above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Location Description']) #Subset not necessary in this case since only one column has N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to explore and filter the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   34|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Distinct description types\n",
    "df.agg(F.countDistinct(F.col(\"Primary Type\")).alias(\"count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        Primary Type|count|\n",
      "+--------------------+-----+\n",
      "|        NON-CRIMINAL|    1|\n",
      "|      NON - CRIMINAL|    2|\n",
      "|           RITUALISM|    3|\n",
      "|NON-CRIMINAL (SUB...|    3|\n",
      "|OTHER NARCOTIC VI...|    4|\n",
      "|   HUMAN TRAFFICKING|    6|\n",
      "|    PUBLIC INDECENCY|    7|\n",
      "|CONCEALED CARRY L...|   26|\n",
      "|           OBSCENITY|   29|\n",
      "|            STALKING|  175|\n",
      "|        INTIMIDATION|  189|\n",
      "|          KIDNAPPING|  309|\n",
      "|LIQUOR LAW VIOLATION|  458|\n",
      "|               ARSON|  688|\n",
      "|            HOMICIDE| 1030|\n",
      "|         SEX OFFENSE| 1038|\n",
      "| CRIM SEXUAL ASSAULT| 1729|\n",
      "|INTERFERENCE WITH...| 1856|\n",
      "|            GAMBLING| 2072|\n",
      "|OFFENSE INVOLVING...| 2487|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Primary Type\").count().sort(F.col(\"count\").asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+\n",
      "|   Primary Type| count|\n",
      "+---------------+------+\n",
      "|      NARCOTICS|125473|\n",
      "|        BATTERY| 90271|\n",
      "|          THEFT| 51405|\n",
      "|CRIMINAL DAMAGE| 37010|\n",
      "|        ASSAULT| 27154|\n",
      "+---------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Primary Type\").count().sort(F.col(\"count\").desc()).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is very unbalanced, and hence we should put the ones with very small values into OTHER CRIMES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of target values below a threshold (e.g. 1000 counts)\n",
    "\n",
    "rename_vals = df.groupBy(\"Primary Type\").count().filter(\n",
    " (F.col(\"count\") < 1000)).select(\"Primary Type\").rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#User defined function which handles all rows below a threshold\n",
    "@udf(StringType())\n",
    "def balance_target(label):\n",
    "    if label in rename_vals:\n",
    "        return \"OTHER CRIMES\"\n",
    "    else:\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('y', balance_target(F.col('Primary Type'))).drop(\"Primary Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Day: date (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Minute: integer (nullable = true)\n",
      " |-- DayOfMonth: integer (nullable = true)\n",
      " |-- DayOfYear: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- WeekOfYear: integer (nullable = true)\n",
      " |-- Quarter: integer (nullable = true)\n",
      " |-- Beat: string (nullable = true)\n",
      " |-- Location Description: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Year should be an integer, and Day should be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"_Year\", df.Year.cast('integer')).drop(*[\"Day\",\"Year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for machine learning\n",
    "\n",
    "### Index categorical data\n",
    "The StringIndexer will encode a column of strings to a column of indices in the range [0,#labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"Beat\", \"Location Description\"]\n",
    "\n",
    "#Index feature columns\n",
    "indexers = [ StringIndexer(inputCol=cat_col, outputCol=\"{}_idx\".format(cat_col),\n",
    "                           handleInvalid = 'error') for cat_col in categorical_cols] \n",
    "\n",
    "#Index target column too since it is categorical\n",
    "target_indexer = [ StringIndexer(inputCol = 'y', outputCol = 'target', handleInvalid = 'error')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoding\n",
    "\n",
    "Spark’s OneHotEncoder One-hot encoding maps a categorical feature, represented as a label index, to a binary vector with at most a single one-value indicating the presence of a specific feature value from among the set of all feature values. For string type input data, it is common to encode categorical features using StringIndexer first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = [OneHotEncoder(dropLast=True,inputCol=idx.getOutputCol(), \n",
    "    outputCol=\"{}_catVec\".format(idx.getOutputCol())) for idx in indexers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector assembler\n",
    "\n",
    "VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. VectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order.\n",
    "\n",
    "We use this to define target and feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"_Year\",\"Month\",\"Hour\",\"Minute\",\"DayOfMonth\",\n",
    "                \"DayOfYear\",\"DayOfWeek\",\"WeekOfYear\",\"Quarter\"] \\\n",
    "+ [enc.getOutputCol() for enc in encoders]\n",
    "\n",
    "assembler = VectorAssembler(inputCols= feature_cols , outputCol=\"Features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are done with transformation steps, and can define a pipeline which executes the steps above.\n",
    "(We could also do PCA, normalize, standardize etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = indexers + encoders + target_indexer  + [assembler])\n",
    "\n",
    "pipeline_model = pipeline.fit(df)\n",
    "final_df = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+\n",
      "|            Features|                  y|target|\n",
      "+--------------------+-------------------+------+\n",
      "|(162,[0,1,3,4,5,6...|CRIM SEXUAL ASSAULT|  18.0|\n",
      "|(162,[0,1,3,4,5,6...|CRIM SEXUAL ASSAULT|  18.0|\n",
      "|(162,[0,1,2,4,5,6...|        SEX OFFENSE|  19.0|\n",
      "|(162,[0,1,2,4,5,6...| DECEPTIVE PRACTICE|  10.0|\n",
      "|(162,[0,1,2,3,4,5...|          NARCOTICS|   0.0|\n",
      "|(162,[0,1,4,5,6,7...| DECEPTIVE PRACTICE|  10.0|\n",
      "|(162,[0,1,3,4,5,6...|        SEX OFFENSE|  19.0|\n",
      "|(162,[0,1,2,3,4,5...|           BURGLARY|   8.0|\n",
      "|(162,[0,1,2,4,5,6...|          NARCOTICS|   0.0|\n",
      "|(162,[0,1,2,4,5,6...|              THEFT|   2.0|\n",
      "|(162,[0,1,2,3,4,5...|  CRIMINAL TRESPASS|  11.0|\n",
      "|(162,[0,1,2,3,4,5...| DECEPTIVE PRACTICE|  10.0|\n",
      "|(162,[0,1,2,4,5,6...| DECEPTIVE PRACTICE|  10.0|\n",
      "|(162,[0,1,3,4,5,6...|        SEX OFFENSE|  19.0|\n",
      "|(162,[0,1,2,4,5,6...|          NARCOTICS|   0.0|\n",
      "|(162,[0,1,2,3,4,5...|          NARCOTICS|   0.0|\n",
      "|(162,[0,1,2,3,4,5...|          NARCOTICS|   0.0|\n",
      "|(162,[0,1,2,3,4,5...| DECEPTIVE PRACTICE|  10.0|\n",
      "|(162,[0,1,2,3,4,5...|          NARCOTICS|   0.0|\n",
      "|(162,[0,1,2,4,5,6...|          NARCOTICS|   0.0|\n",
      "+--------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df.select([\"Features\",\"y\",\"target\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = final_df.randomSplit([0.7, 0.3], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol = 'Features', labelCol = 'target')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+------+--------------------+--------------------+------+----------+--------------------+\n",
      "|_Year|Month|Hour|Minute|Location Description|                   y|target|prediction|         probability|\n",
      "+-----+-----+----+------+--------------------+--------------------+------+----------+--------------------+\n",
      "| 2017|    1|   0|     0|           RESIDENCE|         SEX OFFENSE|  19.0|       1.0|[0.15440129281326...|\n",
      "| 2012|    1|   0|     0|           RESIDENCE| CRIM SEXUAL ASSAULT|  18.0|       1.0|[0.14720829069423...|\n",
      "| 2012|    1|   0|     0|           RESIDENCE|  DECEPTIVE PRACTICE|  10.0|       1.0|[0.14720829069423...|\n",
      "| 2012|    1|   0|     0|           RESIDENCE|OFFENSE INVOLVING...|  14.0|       1.0|[0.14720829069423...|\n",
      "| 2007|    1|   0|     0|           RESIDENCE| CRIM SEXUAL ASSAULT|  18.0|       1.0|[0.14720829069423...|\n",
      "| 2007|    1|   0|     0|           RESIDENCE|               THEFT|   2.0|       1.0|[0.14837137862879...|\n",
      "| 2001|    1|   0|     0|           RESIDENCE| CRIM SEXUAL ASSAULT|  18.0|       1.0|[0.14675795070935...|\n",
      "| 2001|    1|   0|     0|           RESIDENCE| CRIM SEXUAL ASSAULT|  18.0|       1.0|[0.14720829069423...|\n",
      "| 2019|    1|   0|     0|           APARTMENT|     CRIMINAL DAMAGE|   3.0|       1.0|[0.09332509152473...|\n",
      "| 2019|    1|   0|     0|           APARTMENT|OFFENSE INVOLVING...|  14.0|       1.0|[0.10289787379442...|\n",
      "| 2008|    1|   0|     0|           RESIDENCE|               THEFT|   2.0|       1.0|[0.14720829069423...|\n",
      "| 2013|    1|   0|     0|           APARTMENT| CRIM SEXUAL ASSAULT|  18.0|       1.0|[0.11017992920164...|\n",
      "| 2002|    1|   0|     0|           RESIDENCE|               THEFT|   2.0|       1.0|[0.14720829069423...|\n",
      "| 2019|    1|   0|     0|           RESIDENCE|OFFENSE INVOLVING...|  14.0|       1.0|[0.13855733214846...|\n",
      "| 2003|    1|   0|     0|           RESIDENCE|               THEFT|   2.0|       1.0|[0.16145879660681...|\n",
      "| 2014|    1|   0|     0|           RESIDENCE|  DECEPTIVE PRACTICE|  10.0|       1.0|[0.14720829069423...|\n",
      "| 2014|    1|   0|     0|               OTHER|  DECEPTIVE PRACTICE|  10.0|       0.0|[0.21962675033678...|\n",
      "| 2015|    1|   0|     0|           RESIDENCE|OFFENSE INVOLVING...|  14.0|       1.0|[0.14675795070935...|\n",
      "| 2015|    1|   0|     0|               OTHER|  DECEPTIVE PRACTICE|  10.0|       0.0|[0.20917342654377...|\n",
      "| 2004|    1|   0|     0|           RESIDENCE|         SEX OFFENSE|  19.0|       1.0|[0.14720829069423...|\n",
      "+-----+-----+----+------+--------------------+--------------------+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"_Year\",\"Month\",\"Hour\",\"Minute\",\"Location Description\", 'y','target', 'prediction', 'probability').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction| count|\n",
      "+----------+------+\n",
      "|       0.0|101731|\n",
      "|       1.0| 33868|\n",
      "|       2.0|    84|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|target|prediction|\n",
      "+------+----------+\n",
      "|  19.0|       1.0|\n",
      "|  18.0|       1.0|\n",
      "|  10.0|       1.0|\n",
      "|  14.0|       1.0|\n",
      "|  18.0|       1.0|\n",
      "|   2.0|       1.0|\n",
      "|  18.0|       1.0|\n",
      "|  18.0|       1.0|\n",
      "|   3.0|       1.0|\n",
      "|  14.0|       1.0|\n",
      "|   2.0|       1.0|\n",
      "|  18.0|       1.0|\n",
      "|   2.0|       1.0|\n",
      "|  14.0|       1.0|\n",
      "|   2.0|       1.0|\n",
      "|  10.0|       1.0|\n",
      "|  10.0|       0.0|\n",
      "|  14.0|       1.0|\n",
      "|  10.0|       0.0|\n",
      "|  19.0|       1.0|\n",
      "+------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select([\"target\",\"prediction\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22057741217411334"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"target\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.80129213e-02, 4.61913348e-03, 7.04044311e-02, 5.03294660e-02,\n",
       "       1.88002037e-04, 1.29456020e-03, 1.55299801e-03, 2.27934311e-04,\n",
       "       1.52769670e-03, 2.12061628e-02, 2.31569378e-05, 0.00000000e+00,\n",
       "       9.54989725e-05, 6.03632108e-04, 1.10460641e-02, 8.20091796e-03,\n",
       "       6.52470818e-05, 1.87598159e-03, 1.02189752e-03, 2.44306799e-04,\n",
       "       5.04670934e-05, 1.19224780e-04, 9.64368130e-04, 1.02355873e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.57703704e-05, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.65301018e-06, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 2.29458566e-05, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 4.15239142e-02, 2.34971118e-01, 1.41233019e-01,\n",
       "       2.05443448e-01, 1.03535858e-02, 1.61283590e-03, 4.45703081e-02,\n",
       "       0.00000000e+00, 6.18648810e-03, 5.71929426e-03, 3.01252593e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 3.29861186e-02, 2.01954780e-02,\n",
       "       1.29657551e-02, 9.67052487e-03, 1.66038818e-02, 2.21546116e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.95250397e-03, 6.22454825e-03,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 9.88452351e-04,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       3.98988216e-03, 0.00000000e+00, 6.83031873e-04, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 1.07294450e-03, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = rfModel.featureImportances.toArray()\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a lot of features have no impact on the decisions, and hence we should reduce them\n",
    "\n",
    "### Apply PCA \n",
    "\n",
    "Run a PCA algorithm on the vector assembler in order to reduce it. Lets see how the model behaves based on number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with  3  features:  0.19956018704072448\n",
      "Accuracy with  5  features:  0.2314908252937841\n",
      "Accuracy with  8  features:  0.26832962006574956\n",
      "Accuracy with  10  features:  0.26625662856928334\n",
      "Accuracy with  15  features:  0.26166004856406316\n",
      "Accuracy with  20  features:  0.25566383192355396\n",
      "Accuracy with  25  features:  0.2528129498830853\n",
      "Accuracy with  30  features:  0.25477064802534777\n"
     ]
    }
   ],
   "source": [
    "dims = [3,5,8,10,15,20,25,30]\n",
    "for d in dims:\n",
    "    \n",
    "    pca = PCA(k=d, inputCol=\"Features\", outputCol=\"PCA_Features\")\n",
    "\n",
    "    #Add to the ml pipeline\n",
    "    pipeline = Pipeline(stages = indexers + encoders + target_indexer  + [assembler] + [pca])\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    final_df = pipeline_model.transform(df)\n",
    "\n",
    "    #Split train and test\n",
    "    train, test = final_df.randomSplit([0.7, 0.3], seed = 42)\n",
    "\n",
    "    #Create a new random forest classifier based on the PCA features\n",
    "    rf = RandomForestClassifier(featuresCol = 'PCA_Features', labelCol = 'target')\n",
    "    rfModel = rf.fit(train)\n",
    "    predictions = rfModel.transform(test)\n",
    "\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"target\")\n",
    "    print(\"Accuracy with \",d, \" features: \", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the distribution between test and prediction labels. We can see that it is unbalanced and overfitted since most of the target labels isnt predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+-----+\n",
      "|prediction|count|target|count|\n",
      "+----------+-----+------+-----+\n",
      "|      null| null|   8.0| 4193|\n",
      "|       0.0|94154|   0.0|37563|\n",
      "|      null| null|   7.0| 5196|\n",
      "|      null| null|  18.0|  531|\n",
      "|       1.0|31083|   1.0|27254|\n",
      "|      null| null|   4.0| 8156|\n",
      "|      null| null|  11.0| 2426|\n",
      "|      null| null|  14.0|  709|\n",
      "|      null| null|   3.0|10941|\n",
      "|      null| null|  19.0|  314|\n",
      "|       2.0| 9708|   2.0|15365|\n",
      "|      null| null|  17.0|  535|\n",
      "|      10.0|  597|  10.0| 3071|\n",
      "|      null| null|  13.0|  754|\n",
      "|      null| null|   6.0| 5691|\n",
      "|      null| null|  20.0|  324|\n",
      "|      null| null|   5.0| 6174|\n",
      "|      null| null|  15.0|  639|\n",
      "|       9.0|  141|   9.0| 3268|\n",
      "|      null| null|  16.0|  552|\n",
      "+----------+-----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_dist = predictions.groupBy(\"prediction\").count()\n",
    "org_dist = test.groupBy(\"target\").count()\n",
    "\n",
    "pred_dist.join(org_dist,pred_dist.prediction == org_dist.target,how=\"right\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply GridSearch and Kfold CV for model optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-23a21362055d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"PCA_Features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Add to the ml pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexers\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mencoders\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtarget_indexer\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0massembler\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpca\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpipeline_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "pca = PCA(k=8, inputCol=\"Features\", outputCol=\"PCA_Features\")\n",
    "\n",
    "#Add to the ml pipeline\n",
    "pipeline = Pipeline(stages = indexers + encoders + target_indexer  + [assembler] + [pca])\n",
    "pipeline_model = pipeline.fit(df)\n",
    "final_df = pipeline_model.transform(df)\n",
    "\n",
    "#Split train and test\n",
    "train, test = final_df.randomSplit([0.7, 0.3], seed = 42)\n",
    "\n",
    "rf = RandomForestClassifier(featuresCol = 'PCA_Features', labelCol = 'target')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [int(x) for x in np.linspace(start = 2, stop = 5, num = 2)]) \\\n",
    "    .addGrid(rf.maxDepth, [int(x) for x in np.linspace(start = 3, stop = 7, num = 2)]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\",labelCol=\"target\")\n",
    "\n",
    "cv = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train)\n",
    "\n",
    "predictions = cvModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5., 15., 25.])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linspace(start = 5, stop = 25, num = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
