{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def configure_spark(spark_home=None, pyspark_python=None):\n",
    "    spark_home = spark_home or \"/path/to/default/spark/home\"\n",
    "    os.environ['SPARK_HOME'] = spark_home\n",
    "\n",
    "    # Add the PySpark directories to the Python path:\n",
    "    sys.path.insert(1, os.path.join(spark_home, 'python'))\n",
    "    sys.path.insert(1, os.path.join(spark_home, 'python', 'pyspark'))\n",
    "    sys.path.insert(1, os.path.join(spark_home, 'python', 'build'))\n",
    "\n",
    "    # If PySpark isn't specified, use currently running Python binary:\n",
    "    pyspark_python = pyspark_python or sys.executable\n",
    "    os.environ['PYSPARK_PYTHON'] = pyspark_python\n",
    "    \n",
    "configure_spark('/usr/local/spark', '/home/ubuntu/anaconda3/envs/dat500/bin/python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "SparkContext.setSystemProperty('spark.cleaner.periodicGC.interval', '2')\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2400m')\n",
    "SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '2g')\n",
    "SparkContext.setSystemProperty(\"spark.driver.maxResultSize\", \"2g\")\n",
    "\n",
    "sc = pyspark.SparkContext(master='spark://192.168.11.239:7077', appName='type_predicter')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F #avoid conflicts with regular python functions\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler \n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import numpy as np\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pandas as pd\n",
    "import time\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492842"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sqlContext.read.csv(\"/datasets/crimes_cleaned_engineered.csv\", header='true')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Hour: string (nullable = true)\n",
      " |-- DayOfYear: string (nullable = true)\n",
      " |-- DayOfWeek: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- CountHour: string (nullable = true)\n",
      " |-- ProbabilityVector: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert date types\n",
    "We can see that Hour, DayOfWeek and Month is labeles as Strings. We want to convert them to integers. The ProbabilityVector is also a string, so we will remove the endings of it by a UDF, splitting and casting it to an vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ProbabilityVector='[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ProbabilityVector\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = udf(lambda l: Vectors.dense(l), VectorUDT())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def renamer(label):\n",
    "    return label.replace(\"[\",\"\").replace(\"]\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df.withColumn('_ProbabilityVector', renamer(F.col('ProbabilityVector')))\\\n",
    "    .drop(\"ProbabilityVector\")\\\n",
    "    .withColumnRenamed(\"_ProbabilityVector\",\"ProbabilityVector\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df\\\n",
    "    .withColumn(\"castedHour\", df[\"Hour\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"castedMonth\", df[\"Month\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"castedDayOfWeek\", df[\"DayOfWeek\"].cast(IntegerType()))\\\n",
    "    .withColumn(\"castedProbabilityVector\", F.split(F.col(\"ProbabilityVector\"), \",\")\\\n",
    "                .cast(ArrayType(FloatType())))\\\n",
    "    .drop(\"Hour\", \"Month\", \"DayOfWeek\",\"ProbabilityVector\")\\\n",
    "    .withColumnRenamed(\"castedHour\",\"Hour\")\\\n",
    "    .withColumnRenamed(\"castedMonth\",\"Month\")\\\n",
    "    .withColumnRenamed(\"castedDayOfWeek\",\"DayOfWeek\")\\\n",
    "    .withColumnRenamed(\"castedProbabilityVector\",\"ProbabilityVector\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- DayOfYear: string (nullable = true)\n",
      " |-- District: string (nullable = true)\n",
      " |-- y: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- CountHour: string (nullable = true)\n",
      " |-- Hour: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- ProbabilityVector: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast the array to a dense vector using vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df\\\n",
    "       .withColumn(\"castedProbabilityVector\", vectorize(df[\"ProbabilityVector\"]))\\\n",
    "       .drop(\"ProbabilityVector\")\\\n",
    "       .withColumnRenamed(\"castedProbabilityVector\",\"ProbabilityVector\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   ProbabilityVector|\n",
      "+--------------------+\n",
      "|[0.0,0.0,0.0,0.0,...|\n",
      "|[0.0,0.0,0.0,0.0,...|\n",
      "|[0.0,0.0,0.0,0.0,...|\n",
      "|[0.0,0.0,0.0,0.0,...|\n",
      "|[0.0,0.0,0.0,0.0,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"ProbabilityVector\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"District\"]\n",
    "\n",
    "indexers = [ StringIndexer(inputCol=cat_col, outputCol=\"{}_idx\".format(cat_col),\n",
    "                           handleInvalid = 'skip') for cat_col in categorical_cols] \n",
    "\n",
    "target_indexer = [ StringIndexer(inputCol = 'y', outputCol = 'target', handleInvalid = 'skip')]\n",
    "\n",
    "\n",
    "\n",
    "encoders = [OneHotEncoder(dropLast=True,inputCol=idx.getOutputCol(), \n",
    "    outputCol=\"{}_catVec\".format(idx.getOutputCol())) for idx in indexers]\n",
    "\n",
    "\n",
    "fc = [\"Hour\",\"Month\",\"DayOfWeek\", \"ProbabilityVector\"] + [enc.getOutputCol() for enc in encoders]\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols= fc , outputCol=\"Features\")\n",
    "\n",
    "standard_scaler = StandardScaler(inputCol=\"Features\", outputCol=\"scaledFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Model\n",
    "Lets create an example model with the base transformers, and see how it performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = indexers + encoders + target_indexer + [assembler])\n",
    "pipeline_model = pipeline.fit(df)\n",
    "pipeline_df = pipeline_model.transform(df)\n",
    "train = pipeline_df.filter(F.col(\"Year\") < 2015)\n",
    "test = pipeline_df.filter(F.col(\"Year\") >= 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400184"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature vector we pass as input has 76 dimensions, where 52 of them are feature engineered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Features=SparseVector(76, {0: 16.0, 1: 6.0, 2: 6.0, 70: 1.0}))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select(\"Features\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a model and fit on training data. Then we will use the trained model to predict testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(featuresCol = \"Features\",\n",
    "                             labelCol = 'target',\n",
    "                             maxDepth = 10,\n",
    "                             impurity='gini')\n",
    "\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some examples of how it predicted the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------+----------+--------------------+\n",
      "|            Features|      y|target|prediction|         probability|\n",
      "+--------------------+-------+------+----------+--------------------+\n",
      "|(76,[0,1,2,43,70]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[1,2,43,64],[...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,64]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,64]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,4,30,4...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,17,30,...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,46,...|ASSAULT|   4.0|       0.0|[0.48793407886992...|\n",
      "|(76,[0,1,2,43,67]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,67]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,55]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,30,43,...|ASSAULT|   4.0|       2.0|[0.0,0.0,0.494071...|\n",
      "|(76,[0,1,2,43,55]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,62]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,69]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,56]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,63]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,47,...|ASSAULT|   4.0|       1.0|[0.0,0.4878493317...|\n",
      "|(76,[0,1,2,17,30,...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,68]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "|(76,[0,1,2,43,73]...|ASSAULT|   4.0|       4.0|[3.64906427566074...|\n",
      "+--------------------+-------+------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"Features\",'y','target', 'prediction', 'probability').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With imbalanced data, labels with large share often gets too much attention. We will join the predicted count with the original count of the testing data in order to see whether it forgets smaller labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------+-----+\n",
      "|prediction|count|target|count|\n",
      "+----------+-----+------+-----+\n",
      "|       8.0| 2958|   8.0| 2982|\n",
      "|       0.0|18426|   0.0|17713|\n",
      "|       7.0| 2693|   7.0| 2949|\n",
      "|       1.0|14560|   1.0|13651|\n",
      "|       4.0| 5279|   4.0| 5562|\n",
      "|      null| null|  11.0| 1867|\n",
      "|       3.0| 4040|   3.0| 3677|\n",
      "|       2.0| 7833|   2.0| 7979|\n",
      "|      10.0| 2609|  10.0| 2837|\n",
      "|       6.0| 3187|   6.0| 3439|\n",
      "|       5.0| 4532|   5.0| 4847|\n",
      "|       9.0| 8164|   9.0| 5385|\n",
      "|      null| null|  12.0| 1393|\n",
      "+----------+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_dist = predictions.groupBy(\"prediction\").count()\n",
    "org_dist = test.groupBy(\"target\").count()\n",
    "pred_dist.join(org_dist,pred_dist.prediction == org_dist.target,how=\"right\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run an evaluator that computes the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9079037708162249"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\",labelCol=\"target\", metricName=\"accuracy\")\n",
    "\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLlibs lack support for good evaluation methods, so we can collect the results and perform classification report with\n",
    "sklearn if we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_types = np.array(predictions.groupBy(\"y\").count().sort(F.col(\"count\").desc()).select(\"y\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_types = [x.lower() for x in desc_types.flatten()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array(predictions.select('prediction').collect())\n",
    "y_true = np.array(predictions.select('target').collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "              theft       0.94      0.98      0.96     17713\n",
      "            battery       0.93      0.99      0.96     13651\n",
      "    criminal damage       0.96      0.94      0.95      7979\n",
      "            assault       0.90      0.99      0.94      3677\n",
      " deceptive practice       0.97      0.92      0.94      5562\n",
      "      other offense       0.98      0.92      0.95      4847\n",
      "          narcotics       0.97      0.90      0.93      3439\n",
      "           burglary       0.98      0.90      0.94      2949\n",
      "       other crimes       0.94      0.93      0.94      2982\n",
      "motor vehicle theft       0.59      0.90      0.72      5385\n",
      "            robbery       0.98      0.90      0.94      2837\n",
      "  criminal trespass       0.00      0.00      0.00      1867\n",
      "  weapons violation       0.00      0.00      0.00      1393\n",
      "\n",
      "           accuracy                           0.91     74281\n",
      "          macro avg       0.78      0.79      0.78     74281\n",
      "       weighted avg       0.88      0.91      0.89     74281\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/dat500/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred, labels = np.arange(0,len(desc_types)), target_names = desc_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9855413901266811"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = [row['probability'] for row in predictions.collect()]\n",
    "trues = [row['target'] for row in predictions.collect()]\n",
    "correct_count = 0\n",
    "for probs, truth in zip(probabilities, trues):\n",
    "    top_3 = np.argsort(probs)[::-1][:3]\n",
    "    if truth in top_3:\n",
    "        correct_count +=1\n",
    "correct_count/len(trues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tuning\n",
    "We want to find the best Decision Tree model for our data. We can define a set of parameters to search and evaluate with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_3_err(_probs, _trues):\n",
    "    correct_count = 0\n",
    "    for _p, _t in zip(_probs, _trues):\n",
    "        top_3 = np.argsort(_p).flatten()[::-1][:3]\n",
    "        if _t in top_3:\n",
    "            correct_count +=1\n",
    "    return round((1 - (correct_count/len(_trues))) ,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [5,10,20,25,30]\n",
    "impurities = [\"gini\",\"entropy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = indexers + encoders + target_indexer + [assembler])\n",
    "pipeline_model = pipeline.fit(df)\n",
    "pipeline_df = pipeline_model.transform(df)\n",
    "train = pipeline_df.filter(F.col(\"Year\") < 2015)\n",
    "test = pipeline_df.filter(F.col(\"Year\") >= 2016)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    predictionCol=\"prediction\",labelCol=\"target\", metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = {}\n",
    "logger[\"base\"] = []\n",
    "for depth in max_depths:\n",
    "    start = time.time()\n",
    "    dt = DecisionTreeClassifier(featuresCol = \"Features\",\n",
    "                         labelCol = 'target',\n",
    "                         maxDepth = depth,\n",
    "                         impurity= 'gini')\n",
    "\n",
    "    dtModel = dt.fit(train)\n",
    "    predictions = dtModel.transform(test)\n",
    "\n",
    "\n",
    "    execution_time = round((time.time() - start),1)\n",
    "    test_error = round(1 - (evaluator.evaluate(predictions)),4)\n",
    "\n",
    "    y_true = np.array(predictions.select('target').collect())\n",
    "    probs =  np.array(predictions.select('probability').collect())\n",
    "    test_top_3_error = top_3_err(probs,y_true)\n",
    "\n",
    "\n",
    "    logger[\"base\"].append({\n",
    "        \"depth\": depth,\n",
    "        \"imputiry\": 'gini',\n",
    "        \"execution_time\": execution_time,\n",
    "        \"test_error\": test_error,\n",
    "        \"top3_test_error\": test_top_3_error\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base': [{'depth': 5,\n",
       "   'imputiry': 'gini',\n",
       "   'execution_time': 28.7,\n",
       "   'test_error': 0.3037,\n",
       "   'top3_test_error': 0.20305},\n",
       "  {'depth': 10,\n",
       "   'imputiry': 'gini',\n",
       "   'execution_time': 30.3,\n",
       "   'test_error': 0.0921,\n",
       "   'top3_test_error': 0.01446},\n",
       "  {'depth': 20,\n",
       "   'imputiry': 'gini',\n",
       "   'execution_time': 40.2,\n",
       "   'test_error': 0.0587,\n",
       "   'top3_test_error': 0.00905},\n",
       "  {'depth': 25,\n",
       "   'imputiry': 'gini',\n",
       "   'execution_time': 57.2,\n",
       "   'test_error': 0.0589,\n",
       "   'top3_test_error': 0.00895},\n",
       "  {'depth': 30,\n",
       "   'imputiry': 'gini',\n",
       "   'execution_time': 88.1,\n",
       "   'test_error': 0.06,\n",
       "   'top3_test_error': 0.00858}]}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that deeper trees performs best, however 30 depth is less accurate than 20\n",
    "\n",
    "#### PCA pipelines\n",
    "This section will perform tests with different dimensions on different depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [10,25,30]\n",
    "pcas = [5,10,15,25]\n",
    "logger[\"pca\"] = []\n",
    "\n",
    "for k in pcas:\n",
    "    pca = PCA(k = k, inputCol = \"scaledFeatures\", outputCol=\"PCA_Features\")\n",
    "    pipeline = Pipeline(stages = indexers + encoders + target_indexer + [assembler] + [standard_scaler] + [pca])\n",
    "    pipeline_model = pipeline.fit(df)\n",
    "    pipeline_df = pipeline_model.transform(df)\n",
    "    train = pipeline_df.filter(F.col(\"Year\") < 2015)\n",
    "    test = pipeline_df.filter(F.col(\"Year\") >= 2016)\n",
    "    for depth in max_depths:\n",
    "        start = time.time()\n",
    "        \n",
    "        dt = DecisionTreeClassifier(featuresCol = \"PCA_Features\",\n",
    "                             labelCol = 'target',\n",
    "                             maxDepth = depth,\n",
    "                             impurity= 'gini')\n",
    "        \n",
    "        dtModel = dt.fit(train)\n",
    "        predictions = dtModel.transform(test)\n",
    "        execution_time = round((time.time() - start),1)\n",
    "        test_error = round(1 - (evaluator.evaluate(predictions)),4)\n",
    "        y_true = np.array(predictions.select('target').collect())\n",
    "        probs =  np.array(predictions.select('probability').collect())\n",
    "        test_top_3_error = top_3_err(probs,y_true)\n",
    "        logger[\"pca\"].append({\n",
    "            \"depth\": depth,\n",
    "            \"imputiry\": 'gini',\n",
    "            \"execution_time\": execution_time,\n",
    "            \"test_error\": test_error,\n",
    "            \"top3_test_error\": test_top_3_error,\n",
    "            \"dimenstion\": k\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the logged results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('logs/base_dt_search.json', 'w+') as f:\n",
    "    json.dump(logger[\"base\"], f, indent=4)\n",
    "    \n",
    "with open('logs/pca_dt_search.json', 'w+') as f:\n",
    "    json.dump(logger[\"pca\"], f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on best model\n",
    "Use all available training data and train on the best suited model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = {}\n",
    "top_score = 1\n",
    "for i in logger:\n",
    "    for v in logger[i]:\n",
    "        if v[\"test_error\"] < top_score:\n",
    "            top_score = v[\"test_error\"]\n",
    "            top = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth': 20,\n",
       " 'imputiry': 'gini',\n",
       " 'execution_time': 40.2,\n",
       " 'test_error': 0.0587,\n",
       " 'top3_test_error': 0.00905}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = indexers + encoders + target_indexer + [assembler])\n",
    "pipeline_model = pipeline.fit(df)\n",
    "pipeline_df = pipeline_model.transform(df)\n",
    "dt = DecisionTreeClassifier(featuresCol = \"Features\",\n",
    "                     labelCol = 'target',\n",
    "                     maxDepth = 20,\n",
    "                     impurity= 'gini')\n",
    "dtModel = dt.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/dtModel\"\n",
    "dtModel.write().overwrite().save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to load and use it for predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "sameModel = DecisionTreeClassificationModel.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sameModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|prediction|target|\n",
      "+----------+------+\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       0.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       2.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       1.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "|       4.0|   4.0|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.select(\"prediction\",\"target\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
