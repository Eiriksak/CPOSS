{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules\n",
    "When generating prior probabilites for future dates (1 month), we will\n",
    "\n",
    "1. Avg all time, that time of the week, that hour (with all available training data)\n",
    "2. Avg past 365-(future days) days, that day of the week, (+- 1 hour)\n",
    "3. Avg past 60 days-(future days), that day of the week, (+- 1 hour) (with most recent training data)\n",
    "4. Avg +- 30 days, last year, that day of the week, (+- 1 hour)\n",
    "\n",
    "Fetch all time matrix for: each district, dayofweek, hour (most recent one). Calculate based on those matrices.\n",
    "Start by getting a pre calculated past dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def configure_spark(spark_home=None, pyspark_python=None):\n",
    "    spark_home = spark_home or \"/path/to/default/spark/home\"\n",
    "    os.environ['SPARK_HOME'] = spark_home\n",
    "\n",
    "    # Add the PySpark directories to the Python path:\n",
    "    sys.path.insert(1, os.path.join(spark_home, 'python'))\n",
    "    sys.path.insert(1, os.path.join(spark_home, 'python', 'pyspark'))\n",
    "    sys.path.insert(1, os.path.join(spark_home, 'python', 'build'))\n",
    "\n",
    "    # If PySpark isn't specified, use currently running Python binary:\n",
    "    pyspark_python = pyspark_python or sys.executable\n",
    "    os.environ['PYSPARK_PYTHON'] = pyspark_python\n",
    "    \n",
    "configure_spark('/usr/local/spark', '/home/ubuntu/anaconda3/envs/dat500/bin/python')\n",
    "\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "from pyspark import SparkContext\n",
    "\n",
    "SparkContext.setSystemProperty('spark.cleaner.periodicGC.interval', '2')\n",
    "SparkContext.setSystemProperty('spark.executor.memory', '2400m')\n",
    "SparkContext.setSystemProperty('spark.driver.cores', '2')\n",
    "SparkContext.setSystemProperty('spark.driver.memory', '2g')\n",
    "SparkContext.setSystemProperty(\"spark.driver.maxResultSize\", \"2g\")\n",
    "\n",
    "sc = pyspark.SparkContext(master='spark://192.168.11.239:7077', appName='type_predicter')\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F #avoid conflicts with regular python functions\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler \n",
    "import numpy as np\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.linalg import Vectors, MatrixUDT, VectorUDT, DenseMatrix, DenseVector\n",
    "from util import all_time_avg, avg_past_year, avg_past_month, avg_last_year_months\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "tmp = sqlContext.read.csv(\"/datasets/crimes_cleaned_engineered_new.csv\", header='true')\\\n",
    ".select(\"Day\",\"District\", \"Year\", \"DayOfYear\", \"Hour\", \"y\", \"all_time_mat\",\"DayOfWeek\").dropDuplicates()\n",
    "\n",
    "#We do only need the newest records\n",
    "join_df = tmp\\\n",
    ".groupBy(\"District\", \"y\", \"DayOfWeek\", \"Hour\")\\\n",
    ".agg(F.max(\"Day\"))\\\n",
    ".select(F.col(\"District\").alias(\"jDistrict\"), \n",
    "       F.col(\"max(Day)\").alias(\"jDay\"),\n",
    "       F.col(\"Hour\").alias(\"jHour\"),\n",
    "       F.col(\"y\").alias(\"jy\"))\n",
    "\n",
    "tmp = df.join(join_df,\\\n",
    "              ([join_df.jy == tmp.y,\\\n",
    "                join_df.jDistrict == tmp.District,\n",
    "                join_df.jDay == tmp.Day,\n",
    "                join_df.jHour == tmp.Hour]),\\\n",
    "              how='right')\\\n",
    ".select(\"District\", \"Year\", \"DayOfYear\", \"Hour\", \"y\", \"all_time_mat\", \"DayOfWeek\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42997"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+---------+----+------------------+--------------------+---------+\n",
      "|District|Year|DayOfYear|Hour|                 y|        all_time_mat|DayOfWeek|\n",
      "+--------+----+---------+----+------------------+--------------------+---------+\n",
      "|     003|2019|      335|  17|   CRIMINAL DAMAGE|162.0   15.0    2...|        1|\n",
      "|     024|2007|      185|  13| CRIMINAL TRESPASS|185.0   \n",
      "4.0     ...|        4|\n",
      "|     006|2017|      212|  22|           ASSAULT|157.0   5000.0  5...|        2|\n",
      "|     001|2018|      124|  17|           BATTERY|151.0   5000.0  5...|        6|\n",
      "|     017|2009|      186|   4| WEAPONS VIOLATION|186.0   \n",
      "1.0     ...|        1|\n",
      "|     019|2017|      123|  10|DECEPTIVE PRACTICE|22.0    5000.0  5...|        4|\n",
      "|     009|2013|      334|   4|          BURGLARY|246.0   5000.0  \n",
      "...|        7|\n",
      "|     014|2012|      292|   7|           BATTERY|323.0   5000.0  \n",
      "...|        5|\n",
      "|     004|2019|      171|   4|      OTHER CRIMES|171.0   \n",
      "5.0     ...|        5|\n",
      "|     014|2018|      352|  12|DECEPTIVE PRACTICE|25.0    347.0   5...|        3|\n",
      "|     005|2017|      131|  22|   CRIMINAL DAMAGE|55.0    181.0   2...|        5|\n",
      "|     005|2017|      210|  16| CRIMINAL TRESPASS|342.0   6.0     3...|        7|\n",
      "|     002|2016|      193|   2|             THEFT|100.0   5000.0  \n",
      "...|        2|\n",
      "|     002|2016|      316|  16|DECEPTIVE PRACTICE|16.0    5000.0  \n",
      "...|        6|\n",
      "|     010|2019|       59|  14|           BATTERY|71.0    316.0   8...|        5|\n",
      "|     010|2011|      173|   0|           ROBBERY|127.0   5000.0  \n",
      "...|        4|\n",
      "|     010|2019|      142|   7|     OTHER OFFENSE|34.0    174.0   \n",
      "...|        4|\n",
      "|     008|2003|       39|  18| WEAPONS VIOLATION|39.0    \n",
      "7.0     ...|        7|\n",
      "|     005|2017|      121|  10|         NARCOTICS|121.0   5000.0  5...|        2|\n",
      "|     022|2016|      184|   2|           ROBBERY|184.0   \n",
      "7.0     ...|        7|\n",
      "+--------+----+---------+----+------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, Day: date, Month: int, Hour: int, DayOfYear: int, DayOfWeek: int, District: string, y: string, Year: int, CountHour: bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "join_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[District: string, Year: int, DayOfYear: int, Hour: int, y: string, all_time_mat: matrix, DayOfWeek: int]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_types = tmp.select('y').distinct().rdd.map(lambda r: r[0]).collect()\n",
    "districts = tmp.select('District').distinct().rdd.map(lambda r: r[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dayofweek(pandas_time):\n",
    "    mapper = {\n",
    "        0: 2,\n",
    "        1: 3,\n",
    "        2: 4,\n",
    "        3: 5,\n",
    "        4: 6,\n",
    "        5: 7,\n",
    "        6: 1\n",
    "    }\n",
    "    return mapper[pandas_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas datetime dayofweek:  monday (0) - sunday (6)\n",
    "#Pyspark datetime dayofweek:  sunday (1) - monday (7)\n",
    "dates = pd.date_range(start='1/1/2020', end='31/01/2020', freq='h')\n",
    "cols = [\"Date\",\"Hour\",\"DayOfYear\",\"DayOfWeek\", \"District\",\"Year\",\"Month\"]\n",
    "df_list = []\n",
    "for dist in districts:\n",
    "    dist_list = []\n",
    "    doy_list = []\n",
    "    dow_list = []\n",
    "    hr_list = []\n",
    "    yr_list = []\n",
    "    month_list = []\n",
    "    for d in dates:\n",
    "        dist_list.append(dist)\n",
    "        doy_list.append(d.dayofyear)\n",
    "        dow_list.append(convert_dayofweek(d.dayofweek))\n",
    "        hr_list.append(d.hour)\n",
    "        yr_list.append(2020)\n",
    "        month_list.append(1)\n",
    "    \n",
    "    df_tmp = pd.DataFrame(data = np.array([dates, hr_list, doy_list, dow_list, dist_list, yr_list, month_list]).T,\\\n",
    "                          columns=cols)\n",
    "    df_list.append(df_tmp)\n",
    "\n",
    "future_df = pd.concat(df_list,sort=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>DayOfYear</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>District</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>009</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>009</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-01 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>009</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-01 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>009</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-01 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>009</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date Hour DayOfYear DayOfWeek District  Year Month\n",
       "0 2020-01-01 00:00:00    0         1         4      009  2020     1\n",
       "1 2020-01-01 01:00:00    1         1         4      009  2020     1\n",
       "2 2020-01-01 02:00:00    2         1         4      009  2020     1\n",
       "3 2020-01-01 03:00:00    3         1         4      009  2020     1\n",
       "4 2020-01-01 04:00:00    4         1         4      009  2020     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>District</th>\n",
       "      <th>Year</th>\n",
       "      <th>DayOfYear</th>\n",
       "      <th>Hour</th>\n",
       "      <th>y</th>\n",
       "      <th>all_time_mat</th>\n",
       "      <th>DayOfWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>018</td>\n",
       "      <td>2019</td>\n",
       "      <td>191</td>\n",
       "      <td>21</td>\n",
       "      <td>ASSAULT</td>\n",
       "      <td>DenseMatrix([[1.910e+02, 3.380e+02],\\n        ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>011</td>\n",
       "      <td>2009</td>\n",
       "      <td>108</td>\n",
       "      <td>12</td>\n",
       "      <td>OTHER CRIMES</td>\n",
       "      <td>DenseMatrix([[3.260e+02, 3.200e+01, 5.300e+01]...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>024</td>\n",
       "      <td>2007</td>\n",
       "      <td>185</td>\n",
       "      <td>13</td>\n",
       "      <td>CRIMINAL TRESPASS</td>\n",
       "      <td>DenseMatrix([[1.850e+02],\\n             [4.000...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003</td>\n",
       "      <td>2008</td>\n",
       "      <td>177</td>\n",
       "      <td>4</td>\n",
       "      <td>ROBBERY</td>\n",
       "      <td>DenseMatrix([[2.600e+02, 5.000e+03],\\n        ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>017</td>\n",
       "      <td>2009</td>\n",
       "      <td>186</td>\n",
       "      <td>4</td>\n",
       "      <td>WEAPONS VIOLATION</td>\n",
       "      <td>DenseMatrix([[1.860e+02],\\n             [1.000...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  District  Year  DayOfYear  Hour                  y  \\\n",
       "0      018  2019        191    21            ASSAULT   \n",
       "1      011  2009        108    12       OTHER CRIMES   \n",
       "2      024  2007        185    13  CRIMINAL TRESPASS   \n",
       "3      003  2008        177     4            ROBBERY   \n",
       "4      017  2009        186     4  WEAPONS VIOLATION   \n",
       "\n",
       "                                        all_time_mat  DayOfWeek  \n",
       "0  DenseMatrix([[1.910e+02, 3.380e+02],\\n        ...          4  \n",
       "1  DenseMatrix([[3.260e+02, 3.200e+01, 5.300e+01]...          7  \n",
       "2  DenseMatrix([[1.850e+02],\\n             [4.000...          4  \n",
       "3  DenseMatrix([[2.600e+02, 5.000e+03],\\n        ...          4  \n",
       "4  DenseMatrix([[1.860e+02],\\n             [1.000...          1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_most_recent = tmp.toPandas()\n",
    "pd_most_recent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    6168\n",
       "3    6162\n",
       "6    6157\n",
       "5    6137\n",
       "2    6134\n",
       "4    6124\n",
       "1    6115\n",
       "Name: DayOfWeek, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_most_recent.DayOfWeek.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_get_future_probabilities(future_df, past_df):\n",
    "    start = time.time()\n",
    "    df_list = []\n",
    "    for _, r in future_df.iterrows():\n",
    "        #Find matrices for all crime types\n",
    "        _tmp_df = past_df[(past_df[\"Hour\"] == r[\"Hour\"]) & (past_df[\"DayOfWeek\"] == r[\"DayOfWeek\"]) & \\\n",
    "              (past_df[\"District\"] == r[\"District\"])][[\"District\",\"Hour\",\"y\",\"all_time_mat\"]].copy()\n",
    "        _tmp_df[\"DayOfYear\"] = r[\"DayOfYear\"]\n",
    "        _tmp_df[\"Year\"] = r[\"Year\"] \n",
    "        _tmp_df[\"Day\"] = r[\"Date\"]\n",
    "        _tmp_df[\"DayOfWeek\"] = r[\"DayOfWeek\"]\n",
    "        _tmp_df[\"Month\"] = r[\"Month\"]\n",
    "        df_list.append(_tmp_df)\n",
    "        \n",
    "    #Concatenate all available combinations and create a pyspark dataframe from it\n",
    "    _df = sqlContext.createDataFrame(pd.concat(df_list,sort=False).reset_index(drop=True))\n",
    "    print(\"Generated spark base dataframe in: \", round((time.time() - start),1), \" seconds\" )\n",
    "    #Calculate statistics\n",
    "    _df = (_df\\\n",
    "    .withColumn(\"extra\", F.lit(60))\\\n",
    "    .withColumn(\"avg_last_year_months\", avg_last_year_months(\\\n",
    "                                               F.col(\"all_time_mat\"),\n",
    "                                               F.col(\"Year\"),\n",
    "                                               F.col(\"DayOfYear\")))\n",
    "    .withColumn(\"avg_past_month\", avg_past_month(\\\n",
    "                                               F.col(\"all_time_mat\"),\n",
    "                                               F.col(\"Year\"),\n",
    "                                               F.col(\"DayOfYear\"),\n",
    "                                               F.col(\"extra\")))\n",
    "    .withColumn(\"avg_past_year\", avg_past_year(\\\n",
    "                                               F.col(\"all_time_mat\"),\n",
    "                                               F.col(\"Year\"),\n",
    "                                               F.col(\"DayOfYear\"),\n",
    "                                               F.col(\"extra\")))\n",
    "    .withColumn(\"all_time_avg\", all_time_avg(\\\n",
    "                                               F.col(\"all_time_mat\"),\n",
    "                                               F.col(\"Year\"),\n",
    "                                               F.col(\"DayOfYear\"),\n",
    "                                               F.col(\"extra\")))).drop(\"extra\", \"all_time_mat\")\n",
    "    print(\"Completed spard dataframe in : \", round((time.time() - start),1), \" seconds\" )\n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated spark base dataframe in:  283.5  seconds\n",
      "Completed spard dataframe in :  283.7  seconds\n"
     ]
    }
   ],
   "source": [
    "future_probabilities = spark_get_future_probabilities(future_df, pd_most_recent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = future_probabilities.select(\"DayOfYear\",\"DayOfWeek\",\"Year\",\"Day\",\"District\",\"Month\",\"DayOfWeek\",\\\n",
    "                     \"Hour\",\"y\",\"avg_last_year_months\",\"avg_past_month\",\"avg_past_year\",\"all_time_avg\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_list =  [row[\"District\"] for row in r]\n",
    "year_list =  [row[\"Year\"] for row in r]\n",
    "dayofyear_list =  [row[\"DayOfYear\"] for row in r]\n",
    "hour_list =  [row[\"Hour\"] for row in r]\n",
    "day_list =  [row[\"Day\"] for row in r]\n",
    "month_list =  [row[\"Month\"] for row in r]\n",
    "dow_list =  [row[\"DayOfWeek\"] for row in r]\n",
    "y_list =  [row[\"y\"] for row in r]\n",
    "alym_list = [row[\"avg_last_year_months\"] for row in r]\n",
    "apm_list = [row[\"avg_past_month\"] for row in r]\n",
    "apy_list = [row[\"avg_past_year\"] for row in r]\n",
    "ata_list = [row[\"all_time_avg\"] for row in r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_generate_stats(dists, ys, yrs, doys, hrs, alyms, apms, apys, atas, crime_types, districts, years=None):\n",
    "    if not years:\n",
    "        years = np.arange(2001,2020)\n",
    "    leap_years = [2004, 2008, 2012, 2016, 2020]\n",
    "    res = {}\n",
    "    for d in districts:\n",
    "        res[d] = {}\n",
    "        for t in crime_types:\n",
    "            res[d][t] = {}\n",
    "            for year in years:\n",
    "                if year in leap_years:\n",
    "                    res[d][t][year] = {day:{} for day in range(1,367)}\n",
    "                else:\n",
    "                    res[d][t][year] = {day:{} for day in range(1,366)}\n",
    "                    \n",
    "                    \n",
    "    for dist, y, yr, doy, hr, alym, apm, apy, ata in zip(dists, ys, yrs, doys, hrs, alyms, apms, apys, atas):\n",
    "        res[dist][y][yr][doy][hr] = {}\n",
    "        res[dist][y][yr][doy][hr][\"alym\"] = alym\n",
    "        res[dist][y][yr][doy][hr][\"apm\"] = apm\n",
    "        res[dist][y][yr][doy][hr][\"apy\"] = apy\n",
    "        res[dist][y][yr][doy][hr][\"ata\"] = ata    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_res = spark_generate_stats(dist_list,y_list,year_list,dayofyear_list,hour_list,alym_list,\\\n",
    "                           apm_list,apy_list,ata_list,crime_types, districts,years=[2020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability_vectors(district, year, dayofyear, hour, file):\n",
    "    stats = [\"alym\",\"apm\",\"apy\",\"ata\"]\n",
    "    res_vec = []\n",
    "    for dist, yr, doy, hr in zip(district, year, dayofyear, hour):\n",
    "        _vec = []\n",
    "        for s in stats:\n",
    "            for y in file[dist]:\n",
    "                try:\n",
    "                    val = file[dist][y][yr][doy][hr][s]\n",
    "                    _vec.append(val)\n",
    "                except: #No statistics available: append 0\n",
    "                    _vec.append(0)\n",
    "        res_vec.append(Vectors.dense(_vec))\n",
    "    return res_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_probability_vectors = get_probability_vectors(dist_list, year_list, dayofyear_list, hour_list, future_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of future 184519\n",
      "+-------------------+--------+----+--------------------+---------+-----+-------------------+\n",
      "|                  y|District|Hour|   ProbabilityVector|DayOfWeek|Month|                Day|\n",
      "+-------------------+--------+----+--------------------+---------+-----+-------------------+\n",
      "|           BURGLARY|     009|   0|[0.0,0.0,0.0,0.0,...|        4|    1|2020-01-01 00:00:00|\n",
      "|            ASSAULT|     009|   0|[0.0,0.0,0.0,0.0,...|        4|    1|2020-01-01 00:00:00|\n",
      "|       OTHER CRIMES|     009|   0|[0.0,0.0,0.0,0.0,...|        4|    1|2020-01-01 00:00:00|\n",
      "|          NARCOTICS|     009|   0|[0.0,0.0,0.0,0.0,...|        4|    1|2020-01-01 00:00:00|\n",
      "|            ROBBERY|     009|   0|[0.0,0.0,0.0,0.0,...|        4|    1|2020-01-01 00:00:00|\n",
      "|      OTHER OFFENSE|     009|   0|[0.0,0.0,0.0,0.0,...|        4|    1|2020-01-01 00:00:00|\n",
      "|MOTOR VEHICLE THEFT|     009|   0|[0.0,0.0,0.0,0.0,...|        4|    1|2020-01-01 00:00:00|\n",
      "|    CRIMINAL DAMAGE|     009|   0|[0.0,0.0,0.0,0.0,...|        4|    1|2020-01-01 00:00:00|\n",
      "|  CRIMINAL TRESPASS|     009|   0|[0.0,0.0,0.0,0.0,...|        4|    1|2020-01-01 00:00:00|\n",
      "|            BATTERY|     009|   0|[0.0,0.0,0.0,0.0,...|        4|    1|2020-01-01 00:00:00|\n",
      "+-------------------+--------+----+--------------------+---------+-----+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "future = sqlContext\\\n",
    ".createDataFrame(zip(y_list, dist_list, hour_list, future_probability_vectors, dow_list, month_list, day_list),\n",
    "                schema=[\"y\",\"District\", \"Hour\", \"ProbabilityVector\", \"DayOfWeek\",\"Month\",\"Day\"])\n",
    "future.cache()\n",
    "print(\"Length of future\", future.count())\n",
    "future.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future events to be predicted: 184519\n"
     ]
    }
   ],
   "source": [
    "print(\"Future events with duplicates:\",future.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Future events without duplicates: 15862\n"
     ]
    }
   ],
   "source": [
    "future = future.drop_duplicates(subset=['District','Day','Hour',])\n",
    "print(\"Future events without duplicates:\",future.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ProbabilityVector=DenseVector([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0385, 0.0385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0032, 0.0032, 0.0064, 0.0021, 0.0353, 0.0053, 0.0, 0.0011, 0.0011, 0.0053, 0.0021, 0.0011, 0.0]))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future.select(\"ProbabilityVector\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the future\n",
    "This section will predict all possible outcomes of january 2020, based on the probability vectors we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import PCA, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"District\"]\n",
    "\n",
    "indexers = [ StringIndexer(inputCol=cat_col, outputCol=\"{}_idx\".format(cat_col),\n",
    "                           handleInvalid = 'skip') for cat_col in categorical_cols] \n",
    "\n",
    "target_indexer = [ StringIndexer(inputCol = 'y', outputCol = 'target', handleInvalid = 'skip')]\n",
    "\n",
    "\n",
    "\n",
    "encoders = [OneHotEncoder(dropLast=True,inputCol=idx.getOutputCol(), \n",
    "    outputCol=\"{}_catVec\".format(idx.getOutputCol())) for idx in indexers]\n",
    "\n",
    "\n",
    "fc = [\"Hour\",\"Month\",\"DayOfWeek\", \"ProbabilityVector\"] + [enc.getOutputCol() for enc in encoders]\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols= fc , outputCol=\"Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages = indexers + encoders + target_indexer + [assembler])\n",
    "pipeline_model = pipeline.fit(future)\n",
    "pipeline_df = pipeline_model.transform(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassificationModel.load(\"/dtModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_predictions = model.transform(pipeline_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_predictions.select(\"District\",\"Hour\",\"DayOfWeek\",\"prediction\",\"probability\",\"Day\").toPandas().to_csv('future_crimes.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
