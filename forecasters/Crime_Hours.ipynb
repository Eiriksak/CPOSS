{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "def configure_spark(spark_home=None, pyspark_python=None):\n",
    "    spark_home = spark_home or \"/path/to/default/spark/home\"\n",
    "    os.environ['SPARK_HOME'] = spark_home\n",
    "\n",
    "    # Add the PySpark directories to the Python path:\n",
    "    sys.path.insert(1, os.path.join(spark_home, 'python'))\n",
    "    sys.path.insert(1, os.path.join(spark_home, 'python', 'pyspark'))\n",
    "    sys.path.insert(1, os.path.join(spark_home, 'python', 'build'))\n",
    "\n",
    "    # If PySpark isn't specified, use currently running Python binary:\n",
    "    pyspark_python = pyspark_python or sys.executable\n",
    "    os.environ['PYSPARK_PYTHON'] = pyspark_python\n",
    "    \n",
    "configure_spark('/usr/local/spark', '/home/ubuntu/anaconda3/envs/dat500/bin/python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "sc = pyspark.SparkContext(master='spark://192.168.11.239:7077', appName='hourly_forecaster')\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F #avoid conflicts with regular python functions\n",
    "from pyspark.sql.functions import udf, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "os.environ[\"ARROW_PRE_0_15_IPC_FORMAT\"] = \"0\"\n",
    "from fbprophet import Prophet\n",
    "import matplotlib.pyplot as plt\n",
    "from fbprophet.plot import plot_plotly\n",
    "import plotly.offline as py\n",
    "import holidays\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train:  16077\n",
      "Length of test:  661\n"
     ]
    }
   ],
   "source": [
    "df_train = sqlContext.read.csv(\"/datasets/crimes.csv\", header='true')\\\n",
    ".filter( (F.col(\"Year\") < 2020) & (F.col(\"Year\") > 2017)  ).withColumn('Day', F.to_date(\"Date\", 'MM/dd/yyyy hh:mm:ss a'))\\\n",
    ".groupBy(\"Day\",\"District\").count()\n",
    "\n",
    "df_test = sqlContext.read.csv(\"/datasets/crimes.csv\", header='true')\\\n",
    ".filter(F.col(\"Year\") == 2020).withColumn('Day', F.to_date(\"Date\", 'MM/dd/yyyy hh:mm:ss a'))\\\n",
    ".groupBy(\"Day\",\"District\").count()\n",
    "\n",
    "print(\"Length of train: \", df_train.count())\n",
    "print(\"Length of test: \", df_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-----+\n",
      "|       Day|District|count|\n",
      "+----------+--------+-----+\n",
      "|2020-01-30|     009|   22|\n",
      "|2020-01-12|     001|   28|\n",
      "|2020-01-15|     006|   40|\n",
      "|2020-01-01|     004|   34|\n",
      "|2020-01-16|     025|   44|\n",
      "|2020-01-02|     003|   43|\n",
      "|2020-01-20|     010|   40|\n",
      "|2020-01-03|     019|   27|\n",
      "|2020-01-11|     019|   26|\n",
      "|2020-01-09|     025|   27|\n",
      "|2020-01-03|     001|   30|\n",
      "|2020-01-01|     014|   25|\n",
      "|2020-01-14|     024|   28|\n",
      "|2020-01-28|     010|   30|\n",
      "|2020-01-12|     007|   26|\n",
      "|2020-01-24|     002|   22|\n",
      "|2020-01-04|     011|   44|\n",
      "|2020-01-19|     020|    5|\n",
      "|2020-01-21|     022|   17|\n",
      "|2020-01-25|     010|   35|\n",
      "+----------+--------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2019, 12, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Last day to train on\n",
    "df_train.select(F.max(F.col('Day'))).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_US_holidays(start,end,state=\"IL\"):\n",
    "    hdays = holidays.UnitedStates(years=list(np.arange(start,end+1)),state=state).items()\n",
    "    hdays = dict(list(hdays)) #Convert to dict since it is immutable\n",
    "    return {str(k):v for k,v in hdays.items()} #Want Y-m-d format\n",
    "\n",
    "us_hdays = generate_US_holidays(2001,2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def holiday_desc(_date):\n",
    "    if str(_date) in us_hdays.keys():\n",
    "        return us_hdays[str(_date)]\n",
    "    else:\n",
    "        return \"None\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add holiday description as a column on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|       holiday|       Day|\n",
      "+--------------+----------+\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "|New Year's Day|2018-01-01|\n",
      "+--------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (df\\\n",
    "    .withColumn('holiday', holiday_desc(F.col('Day'))).select(\"holiday\",\"Day\").show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"District\", StringType(), True),\n",
    "        StructField(\"count\", StringType(), True),\n",
    "        StructField(\"ds\", DateType(), True),\n",
    "        StructField(\"yhat\", DoubleType(), True)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast data in parallel using Pandas UDFs\n",
    "\n",
    "This strategy is heavily inspired by:\n",
    "https://github.com/AlexWarembourg/Medium/blob/master/Pyspark_Pandas_UDF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "def prophet_udf(df):\n",
    "    \n",
    "    def train_predict_prophet(df, cutoff):\n",
    "        ts_train = (df\n",
    "                    .query('Day <= @cutoff_point')\n",
    "                    .rename(columns={'Day': 'ds', 'count': 'y'})\n",
    "                    .sort_values('ds')\n",
    "                    )\n",
    "        ts_test = (df\n",
    "                   .query('Day > @cutoff_point')\n",
    "                   .rename(columns={'Day': 'ds', 'count': 'y'})\n",
    "                   .sort_values('ds')\n",
    "                   .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                   .drop('y', axis=1)\n",
    "                   )\n",
    "\n",
    "        m = Prophet(yearly_seasonality=True,\n",
    "                    weekly_seasonality=True,\n",
    "                    daily_seasonality=True)\n",
    "        m.fit(ts_train)\n",
    "\n",
    "        # to date\n",
    "        df[\"Day\"] = pd.to_datetime(df[\"Day\"])\n",
    "\n",
    "        \n",
    "        ts_hat = (m.predict(ts_test)[[\"ds\", \"yhat\"]]\n",
    "                  .assign(ds=lambda x: pd.to_datetime(x[\"ds\"]))\n",
    "                  ).merge(ts_test, on=[\"ds\"], how=\"left\")  # merge to retrieve item and store index\n",
    "        return pd.DataFrame(ts_hat, columns=schema.fieldNames())\n",
    "\n",
    "    return train_predict_prophet(df, cutoff_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = sqlContext.read.csv(\"/datasets/crimes.csv\", header='true')\\\n",
    ".filter( (F.col(\"Year\") < 2020) & (F.col(\"Year\") > 2017)  )\\\n",
    ".withColumn('Day', F.to_date(\"Date\", 'MM/dd/yyyy hh:mm:ss a'))\\\n",
    ".withColumn('holiday', holiday_desc(F.col('Day')))\\\n",
    ".select(\"Day\",\"District\",\"holiday\")\\\n",
    ".groupBy(\"Day\",\"District\",\"holiday\")\\\n",
    ".count()\n",
    "\n",
    "\n",
    "df_test = sqlContext.read.csv(\"/datasets/crimes.csv\", header='true')\\\n",
    ".filter(F.col(\"Year\") == 2020)\\\n",
    ".withColumn('Day', F.to_date(\"Date\", 'MM/dd/yyyy hh:mm:ss a'))\\\n",
    ".withColumn('holiday', holiday_desc(F.col('Day')))\\\n",
    ".select(\"Day\",\"District\",\"holiday\")\\\n",
    ".groupBy(\"Day\",\"District\",\"holiday\")\\\n",
    ".count()\n",
    "\n",
    "cutoff_point = df_train.select(F.max(F.col('Day'))).collect()[0][0]\n",
    "#df_test = df_test.withColumn('count', F.lit(None))\n",
    "\n",
    "df = (df_train.union(df_test)).sort(F.col('Day'))\n",
    "\n",
    "\n",
    "predictions = (df\n",
    "              .groupBy(\"District\")\n",
    "              .apply(prophet_udf)\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, lets see an example from district 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+------------------+\n",
      "|District|count|        ds|              yhat|\n",
      "+--------+-----+----------+------------------+\n",
      "|     008| null|2020-01-01|47.985235827081695|\n",
      "|     008| null|2020-01-02|37.820163262207245|\n",
      "|     008| null|2020-01-03|39.696138621564536|\n",
      "|     008| null|2020-01-04|  35.7491284104951|\n",
      "|     008| null|2020-01-05| 35.82730837248379|\n",
      "+--------+-----+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.filter(F.col(\"District\")==\"008\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>ds</th>\n",
       "      <th>lower_window</th>\n",
       "      <th>upper_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2014-07-31</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2014-09-16</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2015-09-16</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2016-09-16</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2017-07-31</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2017-09-16</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2018-09-16</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2019-07-31</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>avocado season</td>\n",
       "      <td>2019-09-16</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           holiday         ds  lower_window  upper_window\n",
       "0   avocado season 2014-07-31            -1             0\n",
       "1   avocado season 2014-09-16            -1             0\n",
       "2   avocado season 2015-07-31            -1             0\n",
       "3   avocado season 2015-09-16            -1             0\n",
       "4   avocado season 2016-07-31            -1             0\n",
       "5   avocado season 2016-09-16            -1             0\n",
       "6   avocado season 2017-07-31            -1             0\n",
       "7   avocado season 2017-09-16            -1             0\n",
       "8   avocado season 2018-07-31            -1             0\n",
       "9   avocado season 2018-09-16            -1             0\n",
       "10  avocado season 2019-07-31            -1             0\n",
       "11  avocado season 2019-09-16            -1             0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avocado_season = pd.DataFrame({\n",
    "  'holiday': 'avocado season',\n",
    "  'ds': pd.to_datetime(['2014-07-31', '2014-09-16', \n",
    "                        '2015-07-31', '2015-09-16',\n",
    "                        '2016-07-31', '2016-09-16',\n",
    "                        '2017-07-31', '2017-09-16',\n",
    "                       '2018-07-31', '2018-09-16',\n",
    "                        '2019-07-31', '2019-09-16']),\n",
    "  'lower_window': -1,\n",
    "  'upper_window': 0,\n",
    "})\n",
    "avocado_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
